# 파이토치_04

### 1.제목: 0.3 파이토치 기초 04, M4팀(김완수,이수호,이영석,박정훈)

**1.1. 소개:**
*   **프로젝트 목적:** 파이토치 심화 학습: 과적합/과소적합 방지 기법 총정리
*   **해결하려는 문제:**
					1. (Overfitting) 방지:  모델이 훈련 데이터에만 과도하게 맞춰져 실제 데이터에 대한 예측 능력이 떨어지는 문제를 해결
					2. (Underfitting) 방지: 모델이 너무 단순하여 데이터의 내재적 구조를 학습하지 못하는 문제를 해결
					3. 학습 속도 향상 및 안정화: 기울기 소실(Vanishing Gradient), 기울기 폭주(Exploding Gradient) 문제를 완화하여 학습을 더 빠르고 안정적으로 만듭니

*   **사용된 주요 기술:** Python (Google Colab 환경)
			- 과적합 방지 기술
				- 데이터 증강 (Data Augmentation):  다양한 데이터를 학습으로 특정 패턴에 과의존 방지
				- 드롭아웃 (Dropout):각 학습 단계에서 다른 신경망을 사용하는 앙상블 효과를 내어 과적합을 줄임
				- 가중치 규제 (Weight Regularization): 손실 함수에 가중치의 크기에 대한 패널티를 추가하여 모델의 복잡도를 제한
						- L1 규제 (Lasso): 일부 가중치를 0으로 만들어 특정 특성을 제거
						- L2 규제 (Ridge): 가중치 값을 작게 만들어 모델이 데이터의 노이즈에 덜 민감하게
				- 배치 정규화 (Batch Normalization): 각 미니배치 단위로 입력 데이터의 평균과 분산을 정규화하여 학습을 안정화(내부 공변량 변화)
            (내부 공변량 변화(internal covariate shift)):딥러닝 모델을 학습할 때, 신경망의 깊은 층으로 갈수록 각 층에 입력되는 데이터의 분포가 계속해서 바뀌는 현상

**1.2. 참여자:**
*   **박정훈:** 학습 내용 이해, 코드 실행 및 기록 담당

### 2. 학습 목차

*   **2.1. 과대적합과 과소적합   **
*   **2.2. 배치 정규화  **
*   **2.3. 가중치 초기화  **
*   **2.4. 정칙화  **
*   **2.5. 데이터 증강 및 변환 **
*   **2.6. 사전 학습된 모델 **

### 3. 문제 정의 및 목표

**3.1. 배경:**
*   아이펠 리서치 15기 FUNDAMANTAL 기초 학습 과정의 일환으로, 데이터 분석 및 AI 모델 개발의 필수적인 기초 역량인 파이토치 심화04를 학습합니다.

**3.2. 목표:**
*   **모델의 일반화 성능을 극대화하고, 학습 과정의 효율성과 안정성을 높임**

### 4. 데이터셋
*   **torchvision.datasets:**등으로 데이터셋 로드
*   **torchvision.transforms:**로 데이터 증강 및 변환 적용
*   **torch.utils.data.DataLoader:**로 데이터로더 생성

**4.1. 데이터셋 출처:**
*   **이미지 분류:**CIFAR-10, ImageNet과 같이 수십만 개의 이미지를 포함하는 대규모 데이터셋.
*   **자연어 처리:**SQuAD, IMDB 등 텍스트 기반의 데이터셋.

**4.2. 데이터셋 설명:**
*   **:**

### 5. 기술 스택 및 라이브러리

**5.1. 주요 기술:**
*   **언어:** Python
*   **주요 라이브러리:**
    *   **PyTorch.:** 딥러닝 모델을 구축하고 학습시키는 핵심 프레임워크..
    *   **Torchvision:** 이미지 관련 작업을 위한 라이브러리. 데이터셋, 모델, 변환(transforms) 기능 등을 제공합니다..
    *   **Hugging Face Transformers:** 트랜스포머 기반의 자연어 처리 모델을 다룰 때 사용됩니다.

*   **사용되는 기술:**
    *   **Batch Normalization:** 각 미니배치의 입력을 평균 0, 분산 1로 정규화하여 학습을 안정시키고 속도를 높입니다.
					- 파이토치 구현: torch.nn.BatchNorm1d, torch.nn.BatchNorm2d 등
		*   **Weight Initialization:** 신경망의 가중치 초기값을 적절하게 설정하여 학습을 안정화합니다. 대표적으로 He 초기화, Xavier 초기화가 사용
					- 파이토치 구현: torch.nn.init 모듈.
    *   **Regularization:** 모델의 복잡도를 제한하여 과적합을 방지
					- L2 정규화(Weight Decay): 손실 함수에 가중치 제곱합을 더해 모델의 가중치를 작게 유지합니다.
					- 드롭아웃(Dropout): 훈련 중 무작위로 일부 뉴런을 비활성화하여 특정 뉴런에 대한 의존도를 낮춥니다.
					- 파이토치 구현: 옵티마이저의 weight_decay 매개변수, torch.nn.Dropout 레이어
    *   **Data Augmentation & Transformation:** 기존 데이터를 인위적으로 변형(회전, 자르기, 뒤집기 등)하여 훈련 데이터의 양을 늘리고 모델의 일반화 능력을 향상
					- 파이토치 구현: torchvision.transforms 모듈.
    *   **Pre-trained Model:** 대규모 데이터셋으로 미리 학습된 모델의 가중치를 가져와 새로운 작업에 맞게 미세 조정(Fine-tuning)하는 전이 학습(Transfer Learning)을 수행
					- 파이토치 구현: torchvision.models에서 ResNet, VGG 등 다양한 모델을 제공


**5.2. 요구 사항:**
*   프로젝트 실행에 필요한 라이브러리 설치 (코랩 환경에서는 대부분 내장).
    ```bash
    # 코랩 셀에서 실행
    !pip install pandas matplotlib seaborn plotly
    ```

### 6. 학습 과정
#. 공통 기본 사항
**1. 데이터 준비:**
	*   **torchvision.datasets:**등으로 데이터셋 로드
	*   **torchvision.transforms:**로 데이터 증강 및 변환 적용
	*   **torch.utils.data.DataLoader:**로 데이터로더 생성
**2. 모델 정의:**
	*   **torch.nn.Module:**을 상속받아 모델 구조 정의
**3. 가중치 초기화:**
	*   **torch.nn.init:**을 활용해 모델의 가중치 초기화
**4. 손실 함수 및 옵티마이저 설정:**
	*   **torch.nn.CrossEntropyLoss (분류), torch.optim.Adam:** 등 설정.
**5. 훈련 루프::**
	*   **순전파(Forward Pass):** 입력 데이터를 모델에 통과시켜 예측값 계산.
	*   **손실 계산:** 예측값과 실제값의 차이 계산.
	*   **역전파(Backward Pass):** 손실에 대한 기울기 계산
	*   **가중치 업데이트:** 옵티마이저로 모델의 가중치 업데이트.
**6. 모델 평가:**
	*   **validation set:** 을 사용해 하이퍼파라미터 튜닝.
**6. 최종 테스트:**
	*   **test set:** 으로 최종 성능 측정.

graph TD
    A[문제 정의] --> B[데이터셋 준비];
    B --> C[모델 구조 정의];
    C --> D[가중치 초기화];
    D --> E[손실 함수 & 옵티마이저 설정];

    subgraph 학습 루프
        E --> F[훈련 시작];
        F --> G[배치 정규화 & 데이터 증강];
        G --> H[순전파];
        H --> I[손실 계산];
        I --> J[역전파 & 가중치 업데이트];
        J --> K[정칙화 적용];
        K --> L[검증 데이터로 성능 평가];
        L -- 과적합 발생? --> M[하이퍼파라미터 조정];
        M --> F;
    end

    L -- 학습 완료 --> N[모델 테스트];
    N --> O[결과 시각화];

**6-1. 학습 정리:** 이번 과정은 딥러닝 모델의 성능과 안정성을 향상시키는 심화 기법을 다룸
										1. 모델이 훈련 데이터에만 너무 맞춰지는 과대적합(Overfitting) 문제를 해결
										2. 학습 효율을 높이는 데 필요한 핵심 기술들을 실습

**6-1. 학습 정리:** 이번 과정
## 1. 과대적합과 과소적합
### 학습 내용
모델의 예측 오차와 학습 곡선(손실, 정확도)을 관찰하여 과대적합(Overfitting)과 과소적합(Underfitting)을 진단하는 방법을 이해합니다.
### 핵심
모델이 데이터의 패턴을 제대로 학습하지 못하면 **과소적합**, 학습 데이터의 잡음까지 외우면 **과대적합**이 발생합니다.
### 사용 기술
*   `matplotlib` 등 시각화 도구를 사용하여 학습 및 검증 손실 곡선 분석

## 2. 배치 정규화

### 학습 내용
`torch.nn.BatchNorm1d`를 통해 미니배치 데이터의 평균이 0, 분산이 1에 가깝게 재조정되는 과정을 확인하고, 학습(train) 시와 추론(eval) 시의 작동 방식 차이를 이해합니다.
### 핵심
데이터 분포를 안정시켜 학습을 빠르게 하고, 기울기 소실 문제를 완화합니다.
### 사용 기술
*   `torch.nn.BatchNorm1d`
*   `model.train()`, `model.eval()`

## 3. 가중치 초기화

### 학습 내용
`torch.nn.init.xavier_uniform_`을 사용한 초기화가 학습의 안정성을 높이는 것을 학습하고, `model.apply()` 메서드를 통해 초기화 과정을 자동화하는 방법을 익힙니다.
### 핵심
학습 초기에 가중치를 적절하게 설정하여 효율적인 학습을 유도하고, `apply()`를 사용해 초기화 과정을 자동화합니다.
### 사용 기술
*   `torch.nn.init.xavier_uniform_`
*   `model.apply()`, `isinstance()`

## 4. 정칙화

### 학습 내용
L1/L2 정칙화 구현 방식과 `weight_decay`를 사용한 간결한 L2 정칙화 적용 방식을 비교합니다. 기울기 폭주를 막는 **그레이디언트 클리핑(`torch.nn.utils.clip_grad_norm_`)**의 작동 방식을 확인합니다.
### 핵심
L1/L2는 모델의 복잡도에 페널티를 부과하고, 클리핑은 기울기의 크기를 제한하여 과적합을 방지하고 학습을 안정화합니다.
### 사용 기술
*   `torch.optim.Adam(weight_decay=...)`
*   `torch.nn.utils.clip_grad_norm_`
*   `sum(p.abs().sum() ...)`

## 5. 데이터 증강 및 변환
### 학습 내용
`nlpaug`와 `torchvision`을 활용하여 텍스트 및 이미지 데이터에 다양한 증강 기법을 적용합니다. 텍스트에서는 역번역, BERT 기반 삽입, 이미지에서는 무작위 회전 및 뒤집기를 실습합니다.
### 핵심
데이터를 인위적으로 늘리고 변형하여 데이터셋의 다양성을 확보하고, 모델의 일반화 성능과 견고성을 향상시킵니다.
### 사용 기술
*   `nlpaug`, `torchvision.transforms.Compose`
*   `nn.Dropout`

## 6. 사전 학습된 모델
### 학습 내용
BERT, GPT와 같은 대규모 사전 학습 언어 모델이 데이터 증강과 전이 학습에 활용되어 데이터 부족 문제를 효과적으로 해결함을 이해합니다.
### 핵심
대규모 데이터셋으로 미리 학습된 모델의 가중치를 활용하여, 적은 데이터로도 빠르게 좋은 성능을 얻습니다.
### 사용 기술
*   `nlpaug.augmenter.word.ContextualWordEmbsAug`
*   `transformers` 라이브러리


### 7. 실행 방법 (Google Colab)

**7.1. 환경 설정 및 파일 준비:**
1.  **코랩 노트북 사용**
2.  **구글드라이브 / 깃허브 연동**

**7.2. 실행:**
1.  **순차적 셀 실행**: 코랩 노트북의 코드 셀들을 학습 노드 순서에 따라 순차적으로 실행합니다.

### 8. 결과 및 시각화

**8.1. 결과 요약:**
*   **손실 및 정확도 곡선::** 훈련 및 검증 데이터에 대한 손실과 정확도를 에폭(epoch)에 따라 그래프로 그려 과적합/과소적합 여부를 판단
*   **혼동 행렬(Confusion Matrix):** 모델의 예측 결과와 실제 값을 비교하여 시각화합니다. 특히 불균형한 데이터셋에서 모델의 성능을 파악하는 데 유용



**8.2. 결과 시각화:**
*   **모델 예측 결과 시각화:**: 이미지 분류의 경우, 모델이 예측한 클래스와 실제 클래스를 함께 보여주어 모델의 성능을 직관적으로 이해

### 9. 회고 및 개선 방향

**9.1. 고민했던 점:**
*   **RuntimeError 디버깅:** nn.Linear 레이어의 입력 차원과 텐서의 shape가 일치하지 않아 발생한 오류를 해결하는 과정에서, 텐서의 형태가 딥러닝 연산에서 얼마나 중요한지 체감
*   **배치 정규화의 작동 방식:**  배치 정규화 후의 값이 항상 0과 1이 아닐 수 있다는 점이 처음에는 혼란스러웠지만, gamma와 beta라는 학습 가능한 매개변수 때문이라는 것을 이해
*   **정칙화 구현 방식:**L2 정칙화를 직접 손실 함수에 더하는 방식과 optimizer의 weight_decay 매개변수를 사용하는 두 가지 방법의 차이를 비교하면서, 후자가 더 간결하고 효율적이라는 것을 알게 됨
*   **하이퍼파라미터 튜닝:**max_norm_value와 같은 하이퍼파라미터의 적절한 값 설정이 모델의 성능과 안정성에 직접적인 영향을 미친다는 점을 고민
*   **데이터 증강의 에너지 소모:**GPT와 같은 대규모 모델을 사용한 데이터 증강이 단순히 편리한 것을 넘어, 엄청난 컴퓨팅 자원을 소모한다는 사실을 깨달음

**9.2. 배운 점:**
*   **정규화의 핵심:** 정규화의 가장 중요한 목적=과적합 방지/모델의 일반화 성능 향상
*   **L1 vs L2:** 가중치를 0으로 만드는 L1(특성 선택)과 0에 가깝게 줄이는 L2(안정성)의 차이를 명확히 구분
*   **Dropout의 중요성:**딥러닝에서 드롭아웃이 과적합 방지와 앙상블 효과를 가져오는 필수적인 기술임을 이해
*   **배치 정규화의 효과:**학습을 안정화하고 속도를 높임

**9.3. 향후 계획:**
*   **심화 학습:** 정규화의 수학적 배경(Norm)을 심도 있게 탐구하여, 개념적 이해 할 수 있도록 시간을 더 할애할 예정.

### 10. 참고 자료

**10.1. 논문:**
*   (이번 학습에서는 참고한 논문이 없습니다.)

**10.2. 참고 웹사이트:**
*   **논문 제목 : Dropout: A Simple Way to Prevent Neural Networks from Overfitting**: https://jmlr.org/papers/v15/srivastava14a.html.
*   **Batch Normalization 논문 PDF**: https://arxiv.org/pdf/1502.03167.pdf.