# FUNDAMANTAL_05

### 1.제목: 0.3 파이토치 기초 45~159, M4팀(김완수,이수호,이영석,박정훈)

**1.1. 소개:**
*   **프로젝트 목적:** 파이토치 기초 이해
*   **해결하려는 문제:** 정칙화에 대한 이해
*   **사용된 주요 기술:** Python (Google Colab 환경)
			- L1 정규화 (Lasso),L2 정규화 (Ridge),Dropout,Batch Normalization

**1.2. 참여자:**
*   **박정훈:** 학습 내용 이해, 코드 실행 및 기록 담당


### 2. 학습 목차

*   **2.1. 텐서   **
*   **2.2. 가설  **
*   **2.3. 손실함수  **
*   **2.4. 최적화  **
*   **2.5. 데이터세트와 데이터로드 **
*   **2.6. 모델/데이터세트 분리 **
*   **2.7. 모델 저장 및 불러오기 **
*   **2.8. 활성화 함수 **
*   **2.9. 순전파와 역전파 **
*   **2.10.퍼셉스론 **


### 3. 문제 정의 및 목표

**3.1. 배경:**
*   아이펠 리서치 15기 FUNDAMANTAL 기초 학습 과정의 일환으로, 데이터 분석 및 AI 모델 개발의 필수적인 기초 역량인 정칙화(Regularization)를 학습합니다.

**3.2. 목표:**
*   **정칙화(Regularization):**개념을 이해하고 정규화(Normalization)와 구분
*   **차이 설명:** L1 regularization과 L2 regularization의 차이를 설명
*   **실습:** 실습을 통하여 Lp norm, Dropout, Batch Normalization에 대해 학습

### 4. 데이터셋

**4.1. 데이터셋 출처:**
*   **:**

**4.2. 데이터셋 설명:**
*   **:**

**4.3. :**
*

### 5. 기술 스택 및 라이브러리

**5.1. 주요 기술:**
*   **언어:** Python
*   **주요 라이브러리:**
    *   **sklearn.linear_model:** L1 및 L2 정칙화가 적용된 선형 모델들을 제공.
    *   **Lasso:** L1 정칙화가 적용된 선형 회귀 모델.
    *   **Ridge:** L2 정칙화가 적용된 선형 회귀 모델.
    *   **ElasticNet:** L1과 L2를 결합한 선형 회귀 모델.
*   **사용되는 기술:**
    *   **alpha:** asso나 Ridge 모델에서 정칙화의 강도를 조절하는 하이퍼파라미터.
    *   **Dropout:** 신경망의 학습 과정에서 무작위로 일부 뉴런을 비활성화하여 과적합을 방지.
    *   **Batch Normalization:** 미니배치 단위로 데이터 분포를 정규화하여 학습을 안정시키고 빠르게 만듬.

**5.2. 요구 사항:**
*   프로젝트 실행에 필요한 라이브러리 설치 (코랩 환경에서는 대부분 내장).
    ```bash
    # 코랩 셀에서 실행
    !pip install pandas matplotlib seaborn plotly
    ```

### 6. 학습 과정

**6.1. 파이토치 기초 학습 선서 이해:**
1.  **데이터 로드**:` Dataset`과 `DataLoader`를 사용하여 데이터를 준비(학습 시작전 한 번)
2.  **모델 생성**: `nn.Module`을 상속받아 학습할 모델의 구조를 정의하고 인스턴스를 생성
									- `퍼셉트론`은 `nn.Linear` 모듈로 구현되며, 모델의 각 층을 구성하는 기본 단위
									- `활성화 함수`는 퍼셉트론(선형 계층)의 출력에 비선형성을 부여하기 위해 적용됩니다. 이들은 nn.Module 내부의 forward() 메서드에 포함
									- `가설`: 모델 자체가 데이터로부터 출력값을 예측하는 가설을 나타냄
3.  **손실 함수 및 최적화 기법 결정**: 학습 방향을 제시할 `손실 함수` 모델의 가중치를 업데이트할 `옵티마이저`를 선택
			**손실함수 선택 근거** 1. 문제의 유형 : 회귀 or 분류 or 컴퓨터 비전
												     2. 데이터의 특성 : 이상치의 영향 or 클래스 불균형
			**옵티마이저 선택의 근거** 1. 학습의 안정성과 속도: SGD 확률적 경사 하강법 or Adam 적응형 모멘트 추정
																 2. 문제의 복잡성 : 경사 하강법의 문제점 or 적응형 학습률
4.  **학습 루프 반복**: 실제 학습
			1.**데이터 로드**: DataLoader에서 미니 배치(mini-batch) 단위의 데이터를 가져옴
			2.**기울기 초기화**: optimizer.zero_grad()로 이전 배치의 기울기를 0으로 셋팅
			3.**순전파**:모델이 입력 데이터를 받아 예측값을 만듬
					- `가설`: forward() 메서드를 실행해 예측값을 얻는 과정이 가설을 통해 출력값을 얻는 과정
			4.**손실 계산**:예측값과 실제 정답을 비교하여 손실을 계산
			5.**역전파**:loss.backward()로 손실에 대한 모델 매개변수의 기울기를 계산
			6.**매개변수 업데이트**:optimizer.step()으로 기울기를 이용해 모델 가중치를 갱신
5.**기울기 초기화**:학습이 완료된 후, state_dict를 사용하여 모델의 가중치를 저장

**적용:**
	**텐서:** 1단계부터 5단계까지 모든 곳에서 사용
	**퍼셉트론과 활성화 함수**: 2단계 모델 생성 시점에 사용.
	**가설**: 2단계 모델 생성과 순전파 단계에서 사용




2.  `Normalization(정규화)`: 트레이닝을 할 때에 서로 범위가 다른 데이터들을 같은 범위로 바꿔주는 전처리 과정.
2.1.  **0과 1사이의 값으로 분포를 조정**: z-score로 바꾸거나 minmax scaler를 사용.
2.2.  **모든 피처 값의 범위를 동일하게**: 데이터의 분포가 피처(feature) 값의 범위에 의해 왜곡되어 학습에 방해 될때.
    - Iris dataset의 회귀 문제
		- 산점도 시각화
		- normalization : 0-1 minmax_scale
		- regularization : 같은 데이터로 간단한 회귀 문제
		- `sklearn.linear_model`,`LinearRegression`,`sklearn.linear_model,L1, L2 regression`Lasso`,`Ridge`
		- linear regression이 L2 norm과 관련

*   ** Regularization과 Normalization의 차이:**
		- 모델에 패널티 vs 데이터의 스케일을 조정을 통해
		- 과적합을 막고, 일반화 성능을 높임 vs 모델의 학습을 안정/빠르게 높임

**6.2. L1 Regularization:**
*   **L1 regularization (Lasso)의 정의:**
1.  **오차 최소화**: 모델이 예측한 값과 실제 값의 차이(오답 노트)를 줄임
2.  **벌점 최소화**: 특성들의 영향력 크기(|βj|)의 합에 벌점(λ)을 부과하여 줄입니다.
										 이 과정에서 모델은 벌점을 피하기 위해 중요하지 않은 특성들의 영향력을 정확히 0으로 만듬

		컬럼 수가 많은 데이터에서의 L1 regularization 비교
		- Iris 데이터는 특성이 총 4개로 컬럼 수가 너무 적으니 wine dataset을 이용
    결과 분석
		- linear regression에서는 모든 컬럼의 가중치가 0이 아님
		- L1 regularization에서는 총 13개 중 7개를 제외한 나머지의 값들이 모두 0임
		-  error 부분에서는 큰 차이가 없었음
		-  linear regression과 L1, L2 regularization의 차이 중 하나는 하이퍼파라미터(수식에서는 λ)가 하나 더 들어간다는 것이고, 그 값에 따라 error에 영향을 미치게 됨

**6.3. L2 Regularization:**
*   **L2 Regularization(Ridge)의 정의:**
1.  **오차 최소화**: 모델이 예측한 값과 실제 값의 차이를 줄임
2.  **벌점 최소화**: 모델의 복잡성을 나타내는 가중치의 제곱합에 벌점을 부과
										 하지 않은 특성들의 영향력을 정확히 0의 근사 값으로 만듬

3. **L1 Regularization과 L2 Regularization의 차이**: 벌점 계산하는 방식
*   **벌점 계산:** L1.(Lasso): 가중치 절댓값의 합에 비례하는 벌점
									 L2.(Ridge): 가중치 제곱의 합에 비례하는 벌점
*   **벌점 효과:** L1.(Lasso): 중요하지 않은 가중치를 0으로 만들어 특성을 제거
									 L2.(Ridge): 모든 가중치를 0에 가깝게 줄여 모델을 안정화

**6.5. Extra : Lp norm:**
*   **Norm의 정의:**  벡터뿐만 아니라 함수, 행렬의 크기를 나타내는 개념
										- 적용 :  L1, L2 정규화는 모델의 복잡도를 제어하기 위해 가중치 벡터의 노름을 최소화하도록 유도
*   **Vector norm:** 벡터의 크기나 길이를 측정하는 방법
										- 적용 : 모델의 예측값과 실제 값의 차이(오차)를 측정할 때
*   **Matrix norm:** 행렬의 '영향력'이나 '전반적인 크기'를 하나의 숫자로 나타냄
										- 적용 : 가중치의 크기가 폭발적으로 커지는 "기울기 폭발(exploding gradients)" 문제를 감지하여 학습이 얼마나 안정적으로 진행되는지 분석할때

**6.5. Dropout:**
*   ** Dropout의 정의:** 확률적으로 랜덤하게 몇 가지의 뉴런만 선택하여 정보를 전달하는 과정.(훈련 과정에서 매번 무작위로 일부 뉴런을 비활성화)
										- 적용 : 단순한 구현으로 과적합 방지, 앙상블 효과, 뉴런의 독점 방지와 같은 강력한 효과를 동시에 얻을 수 있음
                    - 최적화 상태 : train 손실과 validation 손실이 모두 감소하다가, validation 손실이 증가하기 직전의 지점

**6.6. Batch Normalization:**
*   ** Batch Normalization 정의:** (데이터)를 항상 일정한 상태로 만들어 주는 과정
										- 적용 : 일정한 데이터로 빠른 학습 지원, 기울기로 인한 불안정한 문제를 해결, 성능향상, 필수 기술



### 7. 실행 방법 (Google Colab)

**7.1. 환경 설정 및 파일 준비:**
1.  **코랩 노트북 사용**
2.  **구글드라이브 / 깃허브 연동**

**7.2. 실행:**
1.  **순차적 셀 실행**: 코랩 노트북의 코드 셀들을 학습 노드 순서에 따라 순차적으로 실행합니다.

### 8. 결과 및 시각화

**8.1. 결과 요약:**
*   **L1 정규화 (Lasso):** L1 정규화를 적용하면, 중요도가 낮은 특성의 가중치(coefficient)가 정확히 0이 되는 것을 확인
*   **L2 정규화 (Ridge):** 모든 특성의 가중치가 0에 가깝게 줄어들지만, 0이 되지는 않음
*   **Dropout (딥러닝):** 딥러닝 모델의 학습 과정에서 드롭아웃 레이어를 추가했을 때, 훈련 손실과 검증 손실의 차이가 줄어드는 것을 확인

**8.2. 결과 시각화:**
*   **L1/L2 시각화 (개념적):**: 마름모와 원 모양의 제약 조건을 통해 L1이 가중치를 0으로 만드는 이유와 L2가 0에 가깝게 만드는 이유를 시각적으로 이해
*   **Dropout 시각화**: 훈련 손실과 검증 손실 그래프를 통해 드롭아웃이 없는 경우(과적합)와 있는 경우(일반화)의 차이를 명확하게 비교

### 9. 회고 및 개선 방향

**9.1. 고민했던 점:**
*   **추상적 개념:** Regularization, Norm 등 추상적인 수학적 개념이 실제 모델 동작과 어떻게 연결되는지 이해하기 어려웠음.
*   **하이퍼파라미터(\(\lambda \)) 튜닝:**  \(\lambda \) 값에 따라 모델의 성능이 어떻게 변하는지 직관적으로 파악하기 어려웠음
*   **복잡한 개념 및 수식:**복잡한 내용으로 인한 어려움

**9.2. 배운 점:**
*   **정규화의 핵심:** 정규화의 가장 중요한 목적=과적합 방지/모델의 일반화 성능 향상
*   **L1 vs L2:** 가중치를 0으로 만드는 L1(특성 선택)과 0에 가깝게 줄이는 L2(안정성)의 차이를 명확히 구분
*   **Dropout의 중요성:**딥러닝에서 드롭아웃이 과적합 방지와 앙상블 효과를 가져오는 필수적인 기술임을 이해
*   **배치 정규화의 효과:**학습을 안정화하고 속도를 높임

**9.3. 향후 계획:**
*   **심화 학습:** 정규화의 수학적 배경(Norm)을 심도 있게 탐구하여, 개념적 이해 할 수 있도록 시간을 더 할애할 예정.

### 10. 참고 자료

**10.1. 논문:**
*   (이번 학습에서는 참고한 논문이 없습니다.)

**10.2. 참고 웹사이트:**
*   **논문 제목 : Dropout: A Simple Way to Prevent Neural Networks from Overfitting**: https://jmlr.org/papers/v15/srivastava14a.html.
*   **Batch Normalization 논문 PDF**: https://arxiv.org/pdf/1502.03167.pdf.