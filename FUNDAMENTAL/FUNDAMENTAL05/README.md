
# FUNDAMANTAL_05

### 1.제목: 5.Regularization

**1.1. 소개:**
*   **프로젝트 목적:** Regularization의 올바른 이해
*   **해결하려는 문제:** 정칙화에 대한 이해
*   **사용된 주요 기술:** Python (Google Colab 환경)
			- L1 정규화 (Lasso),L2 정규화 (Ridge),Dropout,Batch Normalization

**1.2. 참여자:**
*   **박정훈:** 학습 내용 이해, 코드 실행 및 기록 담당


### 2. 학습 목차

*   **2.1.Regularization과 Normalization   **
*   **2.2. L1 Regularization  **
*   **2.3. L2 Regularization  **
*   **2.4. Extra : Lp norm  **
*   **2.5.  Dropout **
*   **2.6.  Batch Normalization **

### 3. 문제 정의 및 목표

**3.1. 배경:**
*   아이펠 리서치 15기 FUNDAMANTAL 기초 학습 과정의 일환으로, 데이터 분석 및 AI 모델 개발의 필수적인 기초 역량인 정칙화(Regularization)를 학습합니다.

**3.2. 목표:**
*   **정칙화(Regularization):**개념을 이해하고 정규화(Normalization)와 구분
*   **차이 설명:** L1 regularization과 L2 regularization의 차이를 설명
*   **실습:** 실습을 통하여 Lp norm, Dropout, Batch Normalization에 대해 학습

### 4. 데이터셋

**4.1. 데이터셋 출처:**
*   **:** 

**4.2. 데이터셋 설명:**
*   **:** 

**4.3. :**
*   

### 5. 기술 스택 및 라이브러리

**5.1. 주요 기술:**
*   **언어:** Python
*   **주요 라이브러리:**
    *   **Matplotlib:** 기본적인 그래프 생성을 위한 핵심 라이브러리.
    *   **Seaborn:** Matplotlib 기반의 통계 그래프 라이브러리.
    *   **Pandas:** 데이터 핸들링 및 간단한 시각화를 위한 라이브러리.
    *   **Plotly:** (학습 내용에 따라 추가 가능) 인터랙티브 그래프를 위한 라이브러리.
*   **사용되는 기술:**
    *   **데이터 핸들링:** Pandas를 활용한 데이터 호출, 정리, 구조 변경.
    *   **그래프 선택:** 데이터 종류와 분석 목적에 따른 적절한 그래프 선택.
    *   **그래프 커스터마이징:** 목적에 맞게 그래프의 속성 수정.
    *   **결과 해석:** 데이터의 패턴, 추세, 이상치를 파악하여 인사이트 도출.

**5.2. 요구 사항:**
*   프로젝트 실행에 필요한 라이브러리 설치 (코랩 환경에서는 대부분 내장).
    ```bash
    # 코랩 셀에서 실행
    !pip install pandas matplotlib seaborn plotly
    ```

### 6. 학습 과정

**6.1. Regularization과 Normalization:**
1.  `Regularization(정칙화)`: 오버피팅(overfitting)을 막고자 하는 방법.
1.1  train set은 매우 잘 맞히지만, validation/test set은 맞히지 못하는 현상
1.2  가중치에 페널티를 부과, 모델의 복잡도를 조절하는 기술,

2.  `Normalization(정규화)`: 트레이닝을 할 때에 서로 범위가 다른 데이터들을 같은 범위로 바꿔주는 전처리 과정.
2.1.  **0과 1사이의 값으로 분포를 조정**: z-score로 바꾸거나 minmax scaler를 사용.
2.2.  **모든 피처 값의 범위를 동일하게**: 데이터의 분포가 피처(feature) 값의 범위에 의해 왜곡되어 학습에 방해 될때.
    - Iris dataset의 회귀 문제
		- 산점도 시각화
		- normalization : 0-1 minmax_scale
		- regularization : 같은 데이터로 간단한 회귀 문제
		- `sklearn.linear_model`,`LinearRegression`,`sklearn.linear_model,L1, L2 regression`Lasso`,`Ridge`
		- linear regression이 L2 norm과 관련

*   ** Regularization과 Normalization의 차이:**
		- 모델에 패널티 vs 데이터의 스케일을 조정을 통해
		- 과적합을 막고, 일반화 성능을 높임 vs 모델의 학습을 안정/빠르게 높임

**6.2. L1 Regularization:**
*   **L1 regularization (Lasso)의 정의:**
1.  **오차 최소화**: 모델이 예측한 값과 실제 값의 차이(오답 노트)를 줄임
2.  **벌점 최소화**: 특성들의 영향력 크기(|βj|)의 합에 벌점(λ)을 부과하여 줄입니다.
										 이 과정에서 모델은 벌점을 피하기 위해 중요하지 않은 특성들의 영향력을 정확히 0으로 만듬

		컬럼 수가 많은 데이터에서의 L1 regularization 비교
		- Iris 데이터는 특성이 총 4개로 컬럼 수가 너무 적으니 wine dataset을 이용
    결과 분석
		- linear regression에서는 모든 컬럼의 가중치가 0이 아님
		- L1 regularization에서는 총 13개 중 7개를 제외한 나머지의 값들이 모두 0임
		-  error 부분에서는 큰 차이가 없었음
		-  linear regression과 L1, L2 regularization의 차이 중 하나는 하이퍼파라미터(수식에서는 λ)가 하나 더 들어간다는 것이고, 그 값에 따라 error에 영향을 미치게 됨

**6.3. L2 Regularization:**
*   **L2 Regularization(Ridge)의 정의:**
1.  **오차 최소화**: 모델이 예측한 값과 실제 값의 차이를 줄임
2.  **벌점 최소화**: 모델의 복잡성을 나타내는 가중치의 제곱합에 벌점을 부과
										 하지 않은 특성들의 영향력을 정확히 0의 근사 값으로 만듬

3. **L1 Regularization과 L2 Regularization의 차이**: 벌점 계산하는 방식
*   **벌점 계산:** L1.(Lasso): 가중치 절댓값의 합에 비례하는 벌점
									 L2.(Ridge): 가중치 제곱의 합에 비례하는 벌점
*   **벌점 효과:** L1.(Lasso): 중요하지 않은 가중치를 0으로 만들어 특성을 제거
									 L2.(Ridge): 모든 가중치를 0에 가깝게 줄여 모델을 안정화

**6.5. Extra : Lp norm:**
*   **Norm의 정의:**  벡터뿐만 아니라 함수, 행렬의 크기를 나타내는 개념
										- 적용 :  L1, L2 정규화는 모델의 복잡도를 제어하기 위해 가중치 벡터의 노름을 최소화하도록 유도
*   **Vector norm:** 벡터의 크기나 길이를 측정하는 방법
										- 적용 : 모델의 예측값과 실제 값의 차이(오차)를 측정할 때
*   **Matrix norm:** 행렬의 '영향력'이나 '전반적인 크기'를 하나의 숫자로 나타냄
										- 적용 : 가중치의 크기가 폭발적으로 커지는 "기울기 폭발(exploding gradients)" 문제를 감지하여 학습이 얼마나 안정적으로 진행되는지 분석할때

**6.5. Dropout:**
*   ** Dropout의 정의:** 확률적으로 랜덤하게 몇 가지의 뉴런만 선택하여 정보를 전달하는 과정.(훈련 과정에서 매번 무작위로 일부 뉴런을 비활성화)
										- 적용 : 단순한 구현으로 과적합 방지, 앙상블 효과, 뉴런의 독점 방지와 같은 강력한 효과를 동시에 얻을 수 있음
                    - 최적화 상태 : train 손실과 validation 손실이 모두 감소하다가, validation 손실이 증가하기 직전의 지점

**6.6. Batch Normalization:**
*   ** Batch Normalization 정의:** (데이터)를 항상 일정한 상태로 만들어 주는 과정
										- 적용 : 일정한 데이터로 빠른 학습 지원, 기울기로 인한 불안정한 문제를 해결, 성능향상, 필수 기술



### 7. 실행 방법 (Google Colab)

**7.1. 환경 설정 및 파일 준비:**
1.  **코랩 노트북 사용**
2.  **구글드라이브 / 깃허브 연동**

**7.2. 실행:**
1.  **순차적 셀 실행**: 코랩 노트북의 코드 셀들을 학습 노드 순서에 따라 순차적으로 실행합니다.

### 8. 결과 및 시각화

**8.1. 결과 요약:**
*   **L1 정규화 (Lasso):** L1 정규화를 적용하면, 중요도가 낮은 특성의 가중치(coefficient)가 정확히 0이 되는 것을 확인
*   **L2 정규화 (Ridge):** 모든 특성의 가중치가 0에 가깝게 줄어들지만, 0이 되지는 않음
*   **Dropout (딥러닝):** 딥러닝 모델의 학습 과정에서 드롭아웃 레이어를 추가했을 때, 훈련 손실과 검증 손실의 차이가 줄어드는 것을 확인

**8.2. 결과 시각화:**
*   **L1/L2 시각화 (개념적):**: 마름모와 원 모양의 제약 조건을 통해 L1이 가중치를 0으로 만드는 이유와 L2가 0에 가깝게 만드는 이유를 시각적으로 이해
*   **Dropout 시각화**: 훈련 손실과 검증 손실 그래프를 통해 드롭아웃이 없는 경우(과적합)와 있는 경우(일반화)의 차이를 명확하게 비교

### 9. 회고 및 개선 방향

**9.1. 고민했던 점:**
*   **추상적 개념:** Regularization, Norm 등 추상적인 수학적 개념이 실제 모델 동작과 어떻게 연결되는지 이해하기 어려웠음.
*   **하이퍼파라미터(\(\lambda \)) 튜닝:**  \(\lambda \) 값에 따라 모델의 성능이 어떻게 변하는지 직관적으로 파악하기 어려웠음
*   **복잡한 개념 및 수식:**복잡한 내용으로 인한 어려움

**9.2. 배운 점:**
*   **정규화의 핵심:** 정규화의 가장 중요한 목적=과적합 방지/모델의 일반화 성능 향상
*   **L1 vs L2:** 가중치를 0으로 만드는 L1(특성 선택)과 0에 가깝게 줄이는 L2(안정성)의 차이를 명확히 구분
*   **Dropout의 중요성:**딥러닝에서 드롭아웃이 과적합 방지와 앙상블 효과를 가져오는 필수적인 기술임을 이해
*   **배치 정규화의 효과:**학습을 안정화하고 속도를 높임

**9.3. 향후 계획:**
*   **심화 학습:** 정규화의 수학적 배경(Norm)을 심도 있게 탐구하여, 개념적 이해 할 수 있도록 시간을 더 할애할 예정.

### 10. 참고 자료

**10.1. 논문:**
*   (이번 학습에서는 참고한 논문이 없습니다.)

**10.2. 참고 웹사이트:**
*   **논문 제목 : Dropout: A Simple Way to Prevent Neural Networks from Overfitting**: https://jmlr.org/papers/v15/srivastava14a.html.
*   **Batch Normalization 논문 PDF**: https://arxiv.org/pdf/1502.03167.pdf.


 